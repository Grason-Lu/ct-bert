<div id="top" align="center">
<img src=misc/taptap.png width=150 />

CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training
-----------------------------
<h3> |<a href="https://arxiv.org/abs/2307.04308"> Paper </a> | 
<a href="run.sh"> Example </a> |  
<a href="https://drive.google.com/file/d/1zDoH-trPMTGtkpefErozQMulgqioB9Y2/view"> ðŸ˜€ Pre-trained Model </a> |
<a href="https://drive.google.com/file/d/1-2m1tyejUV5_bZduqZw1ZXS1BUSkhzVl/view?usp=drive_link"> ðŸ“• Pre-training Dataset </a>
</h3>
</div>

This is the official Github repository for the paper "CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training" by Chao Ye and Guoshan Lu, etc.

## Overview
Tabular data â€” also known as structured data â€” is one of the most common data forms in existence, thanks to the stable development
and scaled deployment of database systems in the last few decades. At present however, despite the blast brought by large pre-trained
models in other domains such as ChatGPT or SAM, how can we extract common knowledge across tables at a scale that may
eventually lead to generalizable representation for tabular data remains a full blank. In this project, we present **CT-BERT**, a generic and efcient cross-tablepre-training solution.


## Why CT-BERT?
CT-BERT can be fine-tuned for various down stream tabular prediction tasks. For more details, please refer to our <a href="https://arxiv.org/abs/2307.04308"> paper </a>.

## Code

The <a href="run.sh"> </a> demonstrates the overall process of CT-BERT including self-supervised and supervised pre-training, fine-tuning, and prediction.

## Datasets

### Pre-training

We have uploaded our pre-training corpus to to Google Cloud Drive. You can download it from [here](https://drive.google.com/file/d/1zDoH-trPMTGtkpefErozQMulgqioB9Y2/view) and use this [code](https://github.com/Chao-Ye/paperid-437/blob/main/CTBert/load_pretrain_data.py) to load all the datasets.
We will upload to the Huggingface dataset in the next few days.

## Downstream Task Datasets Source
Note that we are careful to guarantee that the tabular datasets used in pre-training and the downstream benchmark datasets are non-overlapping, so there is no data leakage issue. 
pc4  https://www.openml.org/d/1049  
kc1  https://www.openml.org/d/1067  
car  https://archive.ics.uci.edu/dataset/19/car+evaluation  
wilt  https://archive.ics.uci.edu/dataset/285/wilt

higgs  https://www.openml.org/d/44422  
adult  https://archive.ics.uci.edu/dataset/2/adult  
climate  https://archive.ics.uci.edu/dataset/252/climate+model+simulation+crashes  
credit-g  https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data  
vehicle  https://archive.ics.uci.edu/dataset/149/statlog+vehicle+silhouettes  
segment  http://archive.ics.uci.edu/dataset/50/image+segmentation  
amazon  https://www.openml.org/d/44712

satimage  https://archive.ics.uci.edu/dataset/146/statlog+landsat+satellite  
phishing  https://archive.ics.uci.edu/dataset/327/phishing+websites  
mice-protein  https://archive.ics.uci.edu/dataset/342/mice+protein+expression  
cylinder-bands  https://archive.ics.uci.edu/dataset/32/cylinder+bands  

## Citation

If you find this repository useful in your research, please cite our paper:

```bibtex
@misc{ye2023ctbert,
      title={CT-BERT: Learning Better Tabular Representations Through Cross-Table Pre-training}, 
      author={Chao Ye and Guoshan Lu and Haobo Wang and Liyao Li and Sai Wu and Gang Chen and Junbo Zhao},
      year={2023},
      eprint={2307.04308},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```


## Acknowledgement

- [Deepspeed](https://github.com/microsoft/DeepSpeed): We use the deepspeed framework to pre-train our models. We are very grateful for Microsoft's deepspeed framework to make large-scale pre training deployments possible.
